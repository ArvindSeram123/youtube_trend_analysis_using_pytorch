from pyspark.sql import SparkSessionfrom pyspark.sql.functions import col, explode, split, descimport os# Set environment to use UTF-8 encodingos.environ['PYTHONIOENCODING'] = 'utf-8'# Create Spark sessionspark = SparkSession.builder \    .appName("YouTube Local Data Analysis") \    .getOrCreate()# Load data from local JSONdf = spark.read.option("multiLine", True).json("../data/2025-05-02_youtube_data.json")# Show schema for debuggingdf.printSchema()# 1. Top 10 most-viewed categoriestop_categories = df.groupBy("category") \    .sum("views") \    .withColumnRenamed("sum(views)", "total_views") \    .orderBy(desc("total_views")) \    .limit(10)# 2. Keyword frequency in video titleskeywords = df.select(explode(split(col("title"), "\\s+")).alias("word")) \    .groupBy("word") \    .count() \    .orderBy(desc("count")) \    .filter("length(word) > 3") \    .limit(20)# 3. Top tags (Ensure 'tags' is an array column)# Check if 'tags' column exists and is an arrayif 'tags' in df.columns:    exploded_tags = df.select(explode(col("tags")).alias("tag")) \        .groupBy("tag") \        .count() \        .orderBy(desc("count")) \        .limit(20)else:    exploded_tags = spark.createDataFrame([], df.schema)  # Empty DataFrame if no 'tags' column exists# Show resultstop_categories.show()keywords.show()exploded_tags.show()# Optionally save to local CSVs with column headerstop_categories.write.mode("overwrite").option("header","true").csv("../data/output/top_categories")keywords.write.mode("overwrite").option("header","true").csv("../data/output/title_keywords")exploded_tags.write.mode("overwrite").option("header","true").csv("../data/output/top_tags")# Stop Spark sessionspark.stop()